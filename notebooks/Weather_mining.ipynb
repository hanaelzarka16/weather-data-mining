{"cells":[{"cell_type":"markdown","metadata":{"id":"1arzJ248Ms-j"},"source":["## Setup and Imports\n","\n","In this section, we import all the necessary Python libraries used throughout the analysis:\n","\n","- **scikit-learn** for preprocessing, clustering, classification, and evaluation  \n","- **pandas/numpy** for data manipulation  \n","- **matplotlib/seaborn/plotly** for visualization  \n","- **joblib** for saving machine-learning models  \n","\n","We also configure matplotlib for consistent plotting inside Google Colab.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7xmp2zQOWDg","executionInfo":{"status":"ok","timestamp":1765927159346,"user_tz":-120,"elapsed":6,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":8841,"status":"error","timestamp":1765927168200,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"zzvRQBqoOLU0","outputId":"098b3c20-925d-4674-cdcb-1d63ba742676"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0t4h9AjdqfIa","executionInfo":{"status":"aborted","timestamp":1765927168179,"user_tz":-120,"elapsed":9148,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["# Colab-specific installs (run if packages missing)\n","!pip install --quiet kaggle scikit-learn pandas matplotlib seaborn plotly\n","\n","\n","# Standard imports\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.impute import SimpleImputer\n","from sklearn.decomposition import PCA\n","from sklearn.cluster import KMeans, DBSCAN\n","from sklearn.metrics import silhouette_score, davies_bouldin_score, confusion_matrix, classification_report\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neural_network import MLPClassifier\n","import joblib\n","\n","\n","# Plot settings\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (8,5)"]},{"cell_type":"markdown","metadata":{"id":"8A78zt3WM0QS"},"source":["##  Loading the Weather Dataset\n","\n","We load the raw weather dataset from disk to begin our analysis.\n","The dataset contains several meteorological measurements and a binary label indicating whether it rains or not.\n","\n","We display the full dataframe to visually inspect the structure.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VhPbNohBrbcH","executionInfo":{"status":"aborted","timestamp":1765927168181,"user_tz":-120,"elapsed":9149,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["df = pd.read_csv('/content/weather_forecast_data.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9146,"status":"aborted","timestamp":1765927168182,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"WXnp8a7WsAzU"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"ZqJwxf_MM6s3"},"source":["##  Dataset Structure and Missing Values\n","\n","Here we examine:\n","\n","- column names and data types  \n","- number of rows  \n","- memory usage  \n","- how many missing values each column contains  \n","\n","This helps determine preprocessing steps such as imputation or cleaning.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9150,"status":"aborted","timestamp":1765927168189,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"LbXKJWpZtWQ7"},"outputs":[],"source":["# Summary of columns and types\n","df.info()\n","\n","# Count missing values\n","df.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"DGRkSkiNNCII"},"source":["##  Outlier Visualization (Before Cleaning)\n","\n","A boxplot is used to visualize the distribution of each numeric feature and detect potential outliers.\n","\n","Outliers may negatively affect scaling, PCA, and clustering performance, so identifying them early is important.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9149,"status":"aborted","timestamp":1765927168190,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"pJk5lYWLuQFV"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(12, 6))\n","df.boxplot(rot=45)\n","plt.title(\"Boxplot of Weather Dataset Features (Before Outlier Removal)\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"r1enHs_pNHEt"},"source":["##  Removing Outliers Using Z-Score\n","\n","We compute Z-scores for all numeric columns.\n","Values with |Z| > 3 are considered statistical outliers.\n","\n","We then filter out any rows containing such extreme values.\n","\n","This improves overall clustering quality and stabilizes model training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9147,"status":"aborted","timestamp":1765927168191,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"if3GXCfvueHb"},"outputs":[],"source":["from scipy import stats\n","import numpy as np\n","\n","# Select only numeric columns\n","num_df = df.select_dtypes(include=['float64', 'int64'])\n","\n","# Compute Z-scores\n","z_scores = np.abs(stats.zscore(num_df))\n","\n","# Rows where all z-scores < 3\n","df_no_outliers = df[(z_scores < 3).all(axis=1)]\n","\n","df_no_outliers.shape\n"]},{"cell_type":"markdown","metadata":{"id":"_3LDviihNNC8"},"source":["##  Dataset Size Before and After Cleaning\n","\n","We compare the original dataset size with the cleaned one\n","to understand how many outlier rows were removed.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9145,"status":"aborted","timestamp":1765927168192,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"09bn_Jqnuidq"},"outputs":[],"source":["print(\"Original dataset shape:\", df.shape)\n","print(\"After removing outliers:\", df_no_outliers.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"x3DUTaHONUHC"},"source":["##  Outlier Visualization (After Cleaning)\n","\n","We re-plot the boxplots to ensure that extreme values have been effectively removed.\n","\n","This confirms that our dataset is now suitable for scaling and clustering.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1765927168328,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"Spl-AsMculOv"},"outputs":[],"source":["plt.figure(figsize=(12, 6))\n","df_no_outliers.boxplot(rot=45)\n","plt.title(\"Boxplot After Outlier Removal\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"pWsJShXlNY3i"},"source":["##  Extracting the Target Variable\n","\n","The `Rain` column is the binary classification target.\n","\n","We store it as **y** and remove it from the feature set.\n","Only the remaining meteorological features are used for clustering and modeling.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32,"status":"aborted","timestamp":1765927168359,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"XJ_QJrf8wLKD"},"outputs":[],"source":["y = df['Rain']\n","\n","# Drop the target column from the dataset\n","df = df.drop(columns=['Rain'])\n","y"]},{"cell_type":"markdown","metadata":{"id":"SKqIgnV_NdW6"},"source":["##  Feature Scaling (Standardization)\n","\n","We apply **StandardScaler** to normalize the numeric features.\n","\n","Scaling ensures:\n","- all features contribute equally  \n","- PCA and K-Means behave correctly  \n","- distance-based models do not become biased by large-scale features\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":32,"status":"aborted","timestamp":1765927168360,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"HbNeOT0Tun8i"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","# Select numeric columns\n","num_cols = df_no_outliers.select_dtypes(include=['float64', 'int64']).columns\n","\n","# Initialize the scaler\n","scaler = StandardScaler()\n","\n","# Fit and transform\n","scaled_data = scaler.fit_transform(df_no_outliers[num_cols])\n","\n","# Convert back to DataFrame for easier use\n","df_scaled = pd.DataFrame(scaled_data, columns=num_cols)\n","\n","df_scaled.head()\n"]},{"cell_type":"markdown","metadata":{"id":"fEBomS0cNhrS"},"source":["##  PCA Dimensionality Reduction (Unclustered Data)\n","\n","PCA is used to project high-dimensional weather data into 2D.\n","\n","This does NOT perform clustering — it simply helps visualize\n","the structure of the dataset before applying algorithms.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9303,"status":"aborted","timestamp":1765927168360,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"FpPa5TknuvGV"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","pca = PCA(n_components=2)\n","pca_result = pca.fit_transform(df_scaled)\n","\n","plt.figure(figsize=(8,6))\n","plt.scatter(pca_result[:, 0], pca_result[:, 1], s=5)\n","plt.title(\"PCA Projection of Weather Data (Unclustered)\")\n","plt.xlabel(\"PC1\")\n","plt.ylabel(\"PC2\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"1nFuY7KDNqNG"},"source":["##  Feature Engineering\n","\n","We create new derived features to capture additional relationships:\n","\n","- **Wind_Power** (energy approximation from wind speed)\n","- **Humidity_Cloud** (interaction term)\n","- **Pressure_Change** (temporal weather trend)\n","- **Humidity_to_Pressure** and **Cloud_to_Pressure** (ratios)\n","\n","Engineered features often improve clustering and classification performance.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9302,"status":"aborted","timestamp":1765927168361,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"nHq3g1Mtu-T2"},"outputs":[],"source":["import pandas as pd\n","\n","# Rename messy column names for easier coding\n","df = df.rename(columns={\n","    'Temperatu': 'Temperature',\n","    'Wind_Spee': 'Wind_Speed',\n","    'Cloud_Cov': 'Cloud_Cover'\n","})\n","\n","# ----- 1. Wind Power -----\n","df['Wind_Power'] = 0.5 * (df['Wind_Speed'] ** 2)\n","\n","# ----- 2. Humidity × Cloud Interaction -----\n","df['Humidity_Cloud'] = df['Humidity'] * df['Cloud_Cover']\n","\n","# ----- 3. Pressure Drop (difference from previous day) -----\n","df['Pressure_Change'] = df['Pressure'].diff().fillna(0)\n","\n","# ----- 4. Ratios -----\n","df['Humidity_to_Pressure'] = df['Humidity'] / (df['Pressure'] + 1e-5)\n","df['Cloud_to_Pressure'] = df['Cloud_Cover'] / (df['Pressure'] + 1e-5)\n","\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"zteAs4EdNwC5"},"source":["##  Rescaling After Feature Creation\n","\n","After generating new features, we re-apply StandardScaler\n","to ensure the entire feature set remains normalized.\n","\n","This step is necessary because new columns shift the feature distribution.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9293,"status":"aborted","timestamp":1765927168361,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"2JJmvZYnvn6Q"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n","\n","scaler = StandardScaler()\n","df_scaled = pd.DataFrame(scaler.fit_transform(df[numeric_cols]), columns=numeric_cols)\n","\n","df_scaled.head()"]},{"cell_type":"markdown","metadata":{"id":"g7bCDpA60smK"},"source":["#KMEANS\n","\n","##  **K-Means Elbow Method**\n","\n","We test values of **k = 1 to 9** and compute inertia for each model.\n","\n","The elbow point shows where adding more clusters stops improving the model significantly.\n","\n","This helps select the optimal number of clusters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9300,"status":"aborted","timestamp":1765927168371,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"Hd37VUIsvvVR"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","import matplotlib.pyplot as plt\n","\n","inertia = []\n","K_range = range(1, 10)\n","\n","for k in K_range:\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    kmeans.fit(df)\n","    inertia.append(kmeans.inertia_)\n","\n","plt.plot(K_range, inertia, marker='o')\n","plt.xlabel('Number of clusters (k)')\n","plt.ylabel('Inertia')\n","plt.title('Elbow Method')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"ewwVh35kODZC"},"source":["##  Silhouette Score for Cluster Quality\n","\n","We compute silhouette scores for k = 2 to 9.\n","\n","The silhouette score measures:\n","- how compact clusters are  \n","- how separated they are  \n","\n","This helps us confirm the ideal number of clusters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9299,"status":"aborted","timestamp":1765927168372,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"UEusI-QIxeRh"},"outputs":[],"source":["from sklearn.metrics import silhouette_score\n","\n","sil_scores = {}\n","\n","for k in range(2, 10):\n","    kmeans = KMeans(n_clusters=k, random_state=42)\n","    labels = kmeans.fit_predict(df)\n","    sil_scores[k] = silhouette_score(df, labels)\n","\n","sil_scores"]},{"cell_type":"markdown","metadata":{"id":"qqYN6IJqOHal"},"source":["##  Applying K-Means Clustering\n","\n","We select **k = 2** based on evaluation metrics.\n","\n","Cluster labels are added back to the dataframe, allowing us to visualize\n","and analyze the characteristics of each cluster.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9298,"status":"aborted","timestamp":1765927168373,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"FD3b3k60xosP"},"outputs":[],"source":["best_k = 2\n","kmeans = KMeans(n_clusters=best_k, random_state=42)\n","df['Cluster'] = kmeans.fit_predict(df)\n","\n","df[['Temperature','Humidity','Cloud_Cover','Cluster']].head()\n"]},{"cell_type":"markdown","metadata":{"id":"6TlUaUW5OQut"},"source":["##  K-Means Visualization (PCA)\n","\n","We plot the K-Means clusters using PCA-reduced data.\n","\n","This 2D visualization helps us understand the separation between clusters\n","and identify grouping patterns in the weather data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9297,"status":"aborted","timestamp":1765927168374,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"6vc-kQsHxuyS"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","pca = PCA(n_components=2)\n","pca_data = pca.fit_transform(df)\n","\n","plt.scatter(pca_data[:,0], pca_data[:,1], c=df['Cluster'], s=8)\n","plt.title(\"K-Means Clusters (PCA Projection)\")\n","plt.xlabel(\"PC1\")\n","plt.ylabel(\"PC2\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"eqpFTCTDOVHi"},"source":["##  Pairplot Visualization of Clustered Features\n","\n","A pairplot reveals relationships between features for each cluster.\n","\n","This helps uncover:\n","- feature correlations  \n","- cluster shapes  \n","- overlapping regions  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2370yidyDug","executionInfo":{"status":"aborted","timestamp":1765927168384,"user_tz":-120,"elapsed":9305,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","sns.pairplot(df, hue='Cluster', diag_kind='kde', markers='o', height=2)\n","plt.suptitle(\"Pairplot of Features by Cluster\", y=1.02)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"8ASnQdsnOdsh"},"source":["##  Boxplots of Features per Cluster\n","\n","Boxplots show how each cluster differs statistically across key features.\n","\n","This is important for interpreting weather \"types\" created by K-Means.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGgILzq_ybHk","executionInfo":{"status":"aborted","timestamp":1765927168385,"user_tz":-120,"elapsed":9304,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","features = ['Temperature','Humidity','Wind_Speed','Cloud_Cover','Pressure']\n","\n","plt.figure(figsize=(14, 10))\n","\n","for i, col in enumerate(features, 1):\n","    plt.subplot(3, 2, i)\n","    sns.boxplot(data=df, x='Cluster', y=col)\n","    plt.title(f\"{col} by Cluster\")\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"B61KWW0SOhhq"},"source":["##  Cluster Centroid Heatmap\n","\n","We compute the mean of each feature per cluster and plot it in a heatmap.\n","\n","This acts as a \"fingerprint\" for each cluster and shows what weather pattern it represents.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kayecCMYyyA1","executionInfo":{"status":"aborted","timestamp":1765927168386,"user_tz":-120,"elapsed":9303,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["import pandas as pd\n","\n","# Calculate centroids from dataset\n","centroids = df.groupby(\"Cluster\").mean()\n","\n","plt.figure(figsize=(10, 6))\n","sns.heatmap(centroids, annot=True, cmap=\"viridis\")\n","plt.title(\"Cluster Centroids (Mean Values)\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"EEVZYKVgOmoc"},"source":["##  3D PCA Visualization\n","\n","A 3-component PCA is used to visualize the clustered data in 3D space.\n","\n","This offers additional insight into how the datapoints separate beyond 2D.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4G2Ho-IzFMU","executionInfo":{"status":"aborted","timestamp":1765927168393,"user_tz":-120,"elapsed":9308,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["from sklearn.decomposition import PCA\n","from mpl_toolkits.mplot3d import Axes3D\n","import matplotlib.pyplot as plt\n","\n","pca_3d = PCA(n_components=3)\n","pca_data = pca_3d.fit_transform(df)\n","\n","fig = plt.figure(figsize=(8, 6))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","ax.scatter(pca_data[:,0], pca_data[:,1], pca_data[:,2],\n","           c=df['Cluster'], s=20)\n","\n","ax.set_xlabel(\"PC1\")\n","ax.set_ylabel(\"PC2\")\n","ax.set_zlabel(\"PC3\")\n","plt.title(\"3D PCA K-Means Visualization\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"ZaYpF3xDOqt8"},"source":["##  Radar Charts of Cluster Profiles\n","\n","Radar charts display each feature's normalized contribution within each cluster.\n","\n","They provide an intuitive shape-based comparison of weather types.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"30GtdevWzQK-","executionInfo":{"status":"aborted","timestamp":1765927168394,"user_tz":-120,"elapsed":9307,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["import numpy as np\n","\n","centroids = df.groupby(\"Cluster\").mean()\n","\n","# Normalize values for nicer plotting\n","centroids_norm = centroids / centroids.max()\n","\n","labels = centroids_norm.columns\n","num_vars = len(labels)\n","\n","for cluster_id in centroids_norm.index:\n","    values = centroids_norm.loc[cluster_id].values.tolist()\n","    values += values[:1]\n","\n","    angles = np.linspace(0, 2*np.pi, num_vars, endpoint=False).tolist()\n","    angles += angles[:1]\n","\n","    plt.figure(figsize=(6,6))\n","    plt.polar(angles, values, marker='o')\n","    plt.fill(angles, values, alpha=0.25)\n","    print(\"\")\n","    plt.title(f\"Radar Chart for Cluster {cluster_id}\")\n","\n","    plt.xticks(angles[:-1], labels)\n","print(\"\")\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ry3YSQfh0y3j"},"source":["#DBSCAN\n","\n","## **DBSCAN Clustering (Why It Is Not Useful Here)**\n","\n","We attempt DBSCAN, a density-based clustering algorithm.\n","However, DBSCAN is **not suitable for this dataset** for several reasons:\n","\n","###  1. DBSCAN expects natural dense clusters\n","Weather data is continuous, smooth, and evenly distributed →  \n","no dense regions separated by sparse gaps.\n","\n","###  2. DBSCAN detects noise, not classification patterns\n","Our target variable is **binary classification (Rain / No Rain)**.  \n","DBSCAN is unsupervised → it cannot learn decision boundaries.\n","\n","###  3. DBSCAN assigns most points to one giant cluster\n","Because the dataset has no natural density separations,\n","DBSCAN frequently results in:\n","\n","- one large cluster  \n","- all other points labeled as noise (-1)\n","\n","###  4. Silhouette and Davies–Bouldin fail\n","When DBSCAN produces 0 or 1 clusters, evaluation metrics cannot be computed.\n","\n","###  Conclusion  \n","**DBSCAN is not appropriate** for weather forecasting datasets with smooth,\n","continuous numeric distributions.  \n","Therefore, we do not continue clustering with DBSCAN.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I2f4-TPUzapJ","executionInfo":{"status":"aborted","timestamp":1765927168403,"user_tz":-120,"elapsed":9314,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["from sklearn.cluster import DBSCAN\n","\n","dbscan = DBSCAN(eps=0.5, min_samples=5)\n","db_labels = dbscan.fit_predict(df)\n","\n","df['DBSCAN_Cluster'] = db_labels\n","df[['DBSCAN_Cluster']].head()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2opsZ2j11EHs","executionInfo":{"status":"aborted","timestamp":1765927168404,"user_tz":-120,"elapsed":9313,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["import numpy as np\n","\n","unique, counts = np.unique(db_labels, return_counts=True)\n","dict(zip(unique, counts))\n"]},{"cell_type":"markdown","metadata":{"id":"zWppePBwO-Fn"},"source":["##  PCA Visualization of DBSCAN Results\n","\n","We still plot the DBSCAN result to confirm the failure mode:\n","- all points assigned to one huge cluster  \n","- anomalies labeled as -1  \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPxGsvSj1SLE","executionInfo":{"status":"aborted","timestamp":1765927168421,"user_tz":-120,"elapsed":9328,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","pca = PCA(n_components=2)\n","pca_data = pca.fit_transform(df_scaled)\n","\n","plt.scatter(pca_data[:,0], pca_data[:,1], c=db_labels, cmap='plasma', s=10)\n","plt.title(\"DBSCAN Clusters (PCA 2D Projection)\")\n","plt.xlabel(\"PC1\")\n","plt.ylabel(\"PC2\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"2vlnnOb7PDlU"},"source":["##  DBSCAN Evaluation Attempt\n","\n","DBSCAN often produces too few clusters to evaluate.\n","\n","If the number of detected clusters < 2,\n","Silhouette Score and Davies–Bouldin cannot be computed.\n","\n","This confirms DBSCAN is not effective for this dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0V-SvFi91VfC","executionInfo":{"status":"aborted","timestamp":1765927168422,"user_tz":-120,"elapsed":9327,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["from sklearn.metrics import silhouette_score, davies_bouldin_score\n","\n","# Filter out anomalies (-1)\n","mask = db_labels != -1\n","if len(set(db_labels[mask])) > 1:\n","    sil = silhouette_score(X[mask], db_labels[mask])\n","    dbi = davies_bouldin_score(X[mask], db_labels[mask])\n","    print(\"Silhouette Score:\", sil)\n","    print(\"Davies-Bouldin Index:\", dbi)\n","else:\n","    print(\"Not enough clusters for silhouette/DBI evaluation.\")\n"]},{"cell_type":"markdown","metadata":{"id":"LkP6ATewPKIR"},"source":["##  **Future Work**\n","\n","Based on the current project progression and methodology, several improvements can be made to deepen the analysis and increase the predictive capability of the system:\n","\n","###  1. Improved Feature Engineering\n","- Add *time-based features* such as:\n","  - Lagged temperature/humidity values (t−1, t−2)\n","  - Rolling mean / rolling variance\n","  - Seasonal indicators (month, day-of-year, weekend flag)\n","- Derive *meteorological indices*:\n","  - Dew point\n","  - Heat index\n","  - Real-feel temperature\n","  - Wind chill factor\n","- Build weather interaction features:\n","  - Temperature × Humidity  \n","  - Pressure × Wind speed  \n","\n","These features could significantly improve classification performance.\n","\n","---\n","\n","###  2. Advanced Unsupervised Learning\n","- Apply **HDBSCAN**, which handles variable-density clusters better than DBSCAN.\n","- Use **Gaussian Mixture Models (GMM)** to learn soft clusters (probabilistic weather types).\n","- Explore **hierarchical clustering** to see multi-level weather groupings.\n","\n","This may reveal more meaningful weather “types” beyond the hard partitions from K-Means.\n","\n","---\n","\n","###  3. Stronger Supervised Models\n","Using the K-Means clusters or real “Rain/No Rain” labels:\n","\n","- Evaluate **Gradient Boosting models** (XGBoost, CatBoost, LightGBM)\n","- Evaluate **SVM with RBF kernel** for nonlinear decision boundaries  \n","- Apply **Neural Models**:\n","  - MLP with more hidden layers\n","  - 1D CNN for time-based / sensor weather sequences  \n","  - LSTM/GRU for sequential weather predictions  \n","\n","These models typically outperform classical methods on structured weather data.\n","\n","---\n","\n","###  4. Time-Series Modeling  \n","Since weather is inherently sequential:\n","\n","- Apply **ARIMA/SARIMA** to forecast temperature/pressure trends  \n","- Use **Prophet** for seasonal decomposition  \n","- Train **LSTM/Transformer models** to predict future weather conditions  \n","\n","This shifts the project from static classification to real forecasting.\n","\n","---\n","\n","###  5. Better Evaluation and Interpretability\n","- SHAP values for model interpretability  \n","- Partial Dependence Plots (PDPs)  \n","- Weather cluster semantic labeling (e.g., \"Hot–Dry\", \"Humid–Stormy\")  \n","\n","These provide more descriptive cluster meanings rather than just numbers.\n","\n","---\n","\n","###  6. Real-World Deployment\n","- Build a small **Streamlit dashboard** to visualize:\n","  - Clusters  \n","  - Forecasts  \n","  - Daily predictions  \n","- Export predictions as a simple REST API  \n","- Automate daily data collection and prediction via cron jobs  \n","\n","This makes the project practical and interactive.\n","\n","---\n","\n","##  Summary  \n","These extensions would make the project more comprehensive, more predictive, and more aligned with real-world weather analysis tasks.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZvQIDrZi2uR6","executionInfo":{"status":"aborted","timestamp":1765927168442,"user_tz":-120,"elapsed":9347,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9346,"status":"aborted","timestamp":1765927168443,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"9ZL8I21BQBYR"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","\n","# choose numeric features for clustering (exclude label)\n","numeric_cols = df.select_dtypes(include=['float64','int64']).columns.tolist()\n","numeric_cols = [c for c in numeric_cols if c != 'Rain_Encoded']\n","\n","X = df[numeric_cols].copy()\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","pca = PCA(n_components=2, random_state=42)\n","X_pca = pca.fit_transform(X_scaled)\n","\n","# quick sanity\n","print(\"Scaled shape:\", X_scaled.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9360,"status":"aborted","timestamp":1765927168459,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"NVfJ-YNC2cGh"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9360,"status":"aborted","timestamp":1765927168460,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"UA7kkb64GtgG"},"outputs":[],"source":["from sklearn.mixture import GaussianMixture\n","from sklearn.metrics import silhouette_score, davies_bouldin_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# try GMM for k=1..6 (adjust upper bound if you want)\n","k_range = range(1, 7)\n","bics, aics = [], []\n","gmm_models = {}\n","\n","for k in k_range:\n","    print(\"Fitting GMM with k =\", k)\n","    gmm = GaussianMixture(n_components=k, covariance_type='full', random_state=42)\n","    gmm.fit(X_scaled)\n","    bics.append(gmm.bic(X_scaled))\n","    aics.append(gmm.aic(X_scaled))\n","    gmm_models[k] = gmm\n","\n","# Plot BIC and AIC (each is its own plot)\n","plt.figure(figsize=(8,4))\n","plt.plot(list(k_range), bics, marker='o')\n","plt.title('GMM BIC vs #components')\n","plt.xlabel('n_components')\n","plt.ylabel('BIC')\n","plt.show()\n","\n","plt.figure(figsize=(8,4))\n","plt.plot(list(k_range), aics, marker='o')\n","plt.title('GMM AIC vs #components')\n","plt.xlabel('n_components')\n","plt.ylabel('AIC')\n","plt.show()\n","\n","# Pick best k by lowest BIC (common choice)\n","best_k = int(k_range[np.argmin(bics)])\n","print(\"Chosen best_k (lowest BIC):\", best_k)\n","\n","best_gmm = gmm_models[best_k]\n","gmm_labels = best_gmm.predict(X_scaled)        # hard assignments for metrics\n","gmm_probs = best_gmm.predict_proba(X_scaled)   # soft responsibilities\n","\n","# Evaluate (silhouette + Davies-Bouldin) using hard labels if k>=2\n","if best_k >= 2:\n","    sil = silhouette_score(X_scaled, gmm_labels)\n","    dbi = davies_bouldin_score(X_scaled, gmm_labels)\n","else:\n","    sil, dbi = np.nan, np.nan\n","\n","print(f\"GMM metrics (k={best_k}): silhouette={sil:.4f}, davies_bouldin={dbi:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9374,"status":"aborted","timestamp":1765927168476,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"hRot0ioqGv8C"},"outputs":[],"source":["# PCA scatter (each point colored by its GMM hard label)\n","plt.figure(figsize=(8,6))\n","plt.scatter(X_pca[:,0], X_pca[:,1], c=gmm_labels, s=20)\n","plt.title(f'GMM hard labels on PCA (k={best_k})')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9373,"status":"aborted","timestamp":1765927168477,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"b5NaviblG2XM"},"outputs":[],"source":["# stacked bar plot of responsibilities for the first up to 200 records\n","n_display = min(200, gmm_probs.shape[0])\n","indices = np.arange(n_display)\n","bottom = np.zeros(n_display)\n","\n","plt.figure(figsize=(12,4))\n","for comp in range(best_k):\n","    plt.bar(indices, gmm_probs[:n_display, comp], bottom=bottom)\n","    bottom += gmm_probs[:n_display, comp]\n","plt.title(f'GMM soft assignments (first {n_display} samples) — stacked responsibilities')\n","plt.xlabel('sample index (first rows)')\n","plt.ylabel('probability mass')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9387,"status":"aborted","timestamp":1765927168493,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"Bi7IZvTZG6Pg"},"outputs":[],"source":["# component means are in scaled space — inverse transform to original scale\n","centers_scaled = best_gmm.means_\n","centers_orig = scaler.inverse_transform(centers_scaled)\n","\n","plt.figure(figsize=(10, max(2, best_k*0.5)))\n","plt.imshow(centers_orig, aspect='auto')\n","plt.yticks(range(best_k), [f'Comp {i}' for i in range(best_k)])\n","plt.xticks(range(len(numeric_cols)), numeric_cols, rotation=90)\n","plt.title('GMM component means (original feature scale)')\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9386,"status":"aborted","timestamp":1765927168494,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"MtajrvDUG85Q"},"outputs":[],"source":["from scipy.cluster import hierarchy\n","from sklearn.cluster import AgglomerativeClustering\n","import matplotlib.pyplot as plt\n","\n","# compute linkage (Ward)\n","linked = hierarchy.linkage(X_scaled, method='ward')\n","\n","# Dendrogram: truncated to last 150 merges for readability if dataset is large\n","plt.figure(figsize=(12,6))\n","hierarchy.dendrogram(linked, truncate_mode='lastp', p=150)\n","plt.title('Hierarchical Clustering Dendrogram (truncated to last 150 merges)')\n","plt.xlabel('Cluster size')\n","plt.ylabel('Distance')\n","plt.show()\n","\n","# Choose number of clusters for Agglomerative (use best_k as starting point)\n","n_agg = max(2, best_k)\n","agg = AgglomerativeClustering(n_clusters=n_agg)\n","agg_labels = agg.fit_predict(X_scaled)\n","\n","plt.figure(figsize=(8,6))\n","plt.scatter(X_pca[:,0], X_pca[:,1], c=agg_labels, s=20)\n","plt.title(f'Agglomerative Clustering (n_clusters={n_agg}) — PCA 2D')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9399,"status":"aborted","timestamp":1765927168509,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"Hc7qdROQG_d2"},"outputs":[],"source":["df['GMM_Label'] = gmm_labels\n","df['Agg_Label'] = agg_labels\n","df.to_csv('/content//weather_with_advanced_clusters.csv', index=False)\n","print(\"Saved: /content/weather_with_advanced_clusters.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gr0W2DG2Iq8Y","executionInfo":{"status":"aborted","timestamp":1765927168510,"user_tz":-120,"elapsed":9400,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["df = pd.read_csv('/content/weather_with_advanced_clusters.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKKtqIPjKUbG","executionInfo":{"status":"aborted","timestamp":1765927168526,"user_tz":-120,"elapsed":9416,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9415,"status":"aborted","timestamp":1765927168527,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"C7Qvh6ywK37A"},"outputs":[],"source":["y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rt7wfOR4K6sg","executionInfo":{"status":"aborted","timestamp":1765927168538,"user_tz":-120,"elapsed":9425,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9426,"status":"aborted","timestamp":1765927168540,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"gEUu47wTK-wW"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","\n","logreg = LogisticRegression(max_iter=1000)\n","logreg.fit(X_train_scaled, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9456,"status":"aborted","timestamp":1765927168572,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"QfZTWB2eLA9P"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","y_pred = logreg.predict(X_test_scaled)\n","\n","print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n","print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9455,"status":"aborted","timestamp":1765927168573,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"yx_PUP7KLFHX"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","cm = confusion_matrix(y_test, y_pred)\n","\n","plt.figure(figsize=(5,5))\n","plt.imshow(cm, interpolation='nearest')\n","plt.title(\"Confusion Matrix - Logistic Regression\")\n","plt.colorbar()\n","ticks = np.arange(len(cm))\n","plt.xticks([0,1], ['No Rain','Rain'])\n","plt.yticks([0,1], ['No Rain','Rain'])\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","\n","for i in range(cm.shape[0]):\n","    for j in range(cm.shape[1]):\n","        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9454,"status":"aborted","timestamp":1765927168574,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"JIv8HkTENT_l"},"outputs":[],"source":["# 1) Prepare the Rain target and attach to df if necessary, then train Decision Tree\n","import pandas as pd\n","import numpy as np\n","import os\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import joblib\n","\n","# load dataframe (use enriched file if present)\n","path_enriched = '/content/weather_with_advanced_clusters.csv'\n","path_orig = '/content/weather_forecast_data.csv'\n","if os.path.exists(path_enriched):\n","    df = pd.read_csv(path_enriched)\n","    print(\"Loaded:\", path_enriched)\n","elif os.path.exists(path_orig):\n","    df = pd.read_csv(path_orig)\n","    print(\"Loaded:\", path_orig)\n","else:\n","    raise FileNotFoundError(\"No dataset found under /content. Upload your CSV there.\")\n","\n","try:\n","    # 'y' may exist in the notebook as a variable — attach if so and if df lacks 'Rain'\n","    if 'y' in globals() and 'Rain' not in df.columns:\n","        # ensure index alignment: if y is same length and integer index, assign directly\n","        if len(y) == len(df):\n","            df['Rain'] = y.values\n","            print(\"Attached existing 'y' Series to df as df['Rain'].\")\n","        else:\n","            print(\"Found 'y' Series but length != df. Skipping automatic attach. You may need to align indices.\")\n","except Exception as e:\n","    print(\"Skipping attach of 'y' due to:\", e)\n","\n","# If df has a Rain column (strings like 'rain'/'no rain'), encode it to numeric\n","if 'Rain' in df.columns:\n","    df['Rain_Encoded'] = df['Rain'].map({'rain':1, 'no rain':0})\n","    if df['Rain_Encoded'].isnull().any():\n","        # if mapping didn't cover all values, try more robust encoding\n","        df['Rain_Encoded'] = df['Rain'].astype('category').cat.codes\n","    print(\"Created 'Rain_Encoded' from df['Rain']. Unique values:\", df['Rain_Encoded'].unique())\n","else:\n","    raise ValueError(\"No 'Rain' column found in dataframe and no 'y' attached. Please attach your rain labels as df['Rain'].\")\n","\n","# --- Prepare features and target ---\n","target = 'Rain_Encoded'\n","X = df.drop(columns=[target, 'Rain'], errors='ignore')\n","X = X.select_dtypes(include=[np.number])   # keep numeric features only\n","y_enc = df[target].astype(int)\n","\n","print(\"Using features:\", X.columns.tolist())\n","print(\"Target distribution:\\n\", y_enc.value_counts(normalize=False))\n","\n","# --- Train/test split ---\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",")\n","\n","# --- Scaling (optional for tree but consistent across models) ---\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# --- Train Decision Tree ---\n","dt = DecisionTreeClassifier(random_state=42)\n","dt.fit(X_train_scaled, y_train)\n","\n","# --- Evaluate ---\n","y_pred = dt.predict(X_test_scaled)\n","acc = accuracy_score(y_test, y_pred)\n","print(f\"\\nDecision Tree Accuracy: {acc:.4f}\\n\")\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","cm = confusion_matrix(y_test, y_pred)\n","print(\"Confusion Matrix:\\n\", cm)\n","\n","# --- Plot confusion matrix (matplotlib) ---\n","plt.figure(figsize=(5,5))\n","plt.imshow(cm, interpolation='nearest')\n","plt.title(\"Confusion Matrix - Decision Tree\")\n","plt.colorbar()\n","classes = ['No Rain','Rain'] if set(y_enc) <= {0,1} else [str(c) for c in sorted(set(y_enc))]\n","plt.xticks(range(len(classes)), classes)\n","plt.yticks(range(len(classes)), classes)\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","for i in range(cm.shape[0]):\n","    for j in range(cm.shape[1]):\n","        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n","plt.show()\n","\n","# --- Feature importances ---\n","importances = dt.feature_importances_\n","indices = np.argsort(importances)[::-1]\n","top_n = min(20, len(importances))\n","plt.figure(figsize=(8, max(4, top_n*0.3)))\n","plt.bar(range(top_n), importances[indices][:top_n])\n","plt.xticks(range(top_n), [X.columns[i] for i in indices[:top_n]], rotation=90)\n","plt.title(\"Top feature importances (Decision Tree)\")\n","plt.tight_layout()\n","plt.show()\n","\n","# --- Save model and scaler ---\n","joblib.dump({'model':dt, 'scaler':scaler, 'features': X.columns.tolist()}, '/content/decision_tree_model.joblib')\n","print(\"Saved Decision Tree model to /content/decision_tree_model.joblib\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9453,"status":"aborted","timestamp":1765927168575,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"LjG6EE1VO8Ot"},"outputs":[],"source":["# Random Forest training + evaluation (paste into Colab)\n","import os\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","import joblib\n","\n","# 1) Load dataset (prefer enriched file)\n","path_enriched = '/content/weather_with_advanced_clusters.csv'\n","path_orig = '/content/weather_forecast_data.csv'\n","if os.path.exists(path_enriched):\n","    df = pd.read_csv(path_enriched)\n","    print(\"Loaded enriched dataset:\", path_enriched)\n","elif os.path.exists(path_orig):\n","    df = pd.read_csv(path_orig)\n","    print(\"Loaded base dataset:\", path_orig)\n","else:\n","    raise FileNotFoundError(\"No dataset found at /mnt/data. Upload your CSV there.\")\n","\n","# 2) If a standalone 'y' exists in the notebook, attach it to df if df lacks Rain\n","try:\n","    if 'y' in globals() and 'Rain' not in df.columns:\n","        if len(y) == len(df):\n","            df['Rain'] = y.values\n","            print(\"Attached existing 'y' Series to df as df['Rain'].\")\n","        else:\n","            print(\"Found 'y' Series but length != df; not attaching automatically.\")\n","except Exception as e:\n","    print(\"Skipping automatic attach of 'y' due to:\", e)\n","\n","# 3) Ensure Rain_Encoded exists (encode if necessary)\n","if 'Rain' in df.columns and 'Rain_Encoded' not in df.columns:\n","    df['Rain_Encoded'] = df['Rain'].map({'rain':1, 'no rain':0})\n","    if df['Rain_Encoded'].isnull().any():\n","        df['Rain_Encoded'] = df['Rain'].astype('category').cat.codes\n","    print(\"Created 'Rain_Encoded'. Unique values:\", df['Rain_Encoded'].unique())\n","\n","if 'Rain_Encoded' not in df.columns:\n","    raise ValueError(\"No 'Rain_Encoded' target found. Make sure df has a 'Rain' column or a 'y' Series attached.\")\n","\n","# 4) Prepare features and target\n","target = 'Rain_Encoded'\n","X = df.drop(columns=[target, 'Rain'], errors='ignore')\n","X = X.select_dtypes(include=[np.number])  # use numeric features only\n","y_enc = df[target].astype(int)\n","\n","print(\"Features used:\", X.columns.tolist())\n","print(\"Target distribution:\\n\", y_enc.value_counts())\n","\n","# 5) Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",")\n","\n","# 6) Scaling (not required for RF, but we will keep scaler for consistency)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# 7) Train Random Forest (baseline)\n","rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n","rf.fit(X_train_scaled, y_train)\n","\n","# 8) Predict & evaluate\n","y_pred = rf.predict(X_test_scaled)\n","acc = accuracy_score(y_test, y_pred)\n","print(f\"\\nRandom Forest Accuracy: {acc:.4f}\\n\")\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(\"Confusion Matrix:\\n\", cm)\n","\n","# 9) Confusion matrix plot (matplotlib)\n","plt.figure(figsize=(5,5))\n","plt.imshow(cm, interpolation='nearest')\n","plt.title(\"Confusion Matrix - Random Forest\")\n","plt.colorbar()\n","classes = ['No Rain','Rain'] if set(y_enc) <= {0,1} else [str(c) for c in sorted(set(y_enc))]\n","plt.xticks(range(len(classes)), classes)\n","plt.yticks(range(len(classes)), classes)\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","for i in range(cm.shape[0]):\n","    for j in range(cm.shape[1]):\n","        plt.text(j, i, cm[i, j], ha=\"center\", va=\"center\")\n","plt.show()\n","\n","# 10) ROC AUC (binary case)\n","if len(set(y_enc)) == 2:\n","    y_prob = rf.predict_proba(X_test_scaled)[:, 1]\n","    try:\n","        auc = roc_auc_score(y_test, y_prob)\n","        print(f\"ROC AUC: {auc:.4f}\")\n","        fpr, tpr, _ = roc_curve(y_test, y_prob)\n","        plt.figure(figsize=(6,5))\n","        plt.plot(fpr, tpr)\n","        plt.plot([0,1],[0,1], linestyle='--')\n","        plt.title(\"ROC Curve - Random Forest\")\n","        plt.xlabel(\"False Positive Rate\")\n","        plt.ylabel(\"True Positive Rate\")\n","        plt.show()\n","    except Exception as e:\n","        print(\"ROC AUC calculation failed:\", e)\n","\n","# 11) Feature importances (plot top features)\n","importances = rf.feature_importances_\n","indices = np.argsort(importances)[::-1]\n","top_n = min(20, len(importances))\n","\n","plt.figure(figsize=(8, max(4, top_n*0.3)))\n","plt.bar(range(top_n), importances[indices][:top_n])\n","plt.xticks(range(top_n), [X.columns[i] for i in indices[:top_n]], rotation=90)\n","plt.title(\"Top feature importances (Random Forest)\")\n","plt.tight_layout()\n","plt.show()\n","\n","# 12) Save model + scaler + feature list\n","model_path = '/content/random_forest_model.joblib'\n","joblib.dump({'model': rf, 'scaler': scaler, 'features': X.columns.tolist()}, model_path)\n","print(\"Saved Random Forest model to:\", model_path)\n","\n","# 13) Show random sample of test rows with predictions\n","sample_idx = np.random.choice(X_test.index, size=min(10, len(X_test)), replace=False)\n","sample = X.loc[sample_idx].copy()\n","sample['true_Rain'] = y_enc.loc[sample_idx].values\n","sample['pred_Rain'] = rf.predict(scaler.transform(sample[X.columns]))\n","print(\"\\nSample of test rows with predictions:\")\n","print(sample.head(10).to_string())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9458,"status":"aborted","timestamp":1765927168582,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"oYjMyxodRYaH"},"outputs":[],"source":["# Fine-tuned MLP with RandomizedSearchCV - paste & run in Colab\n","import os\n","import time\n","import joblib\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n","from sklearn.inspection import permutation_importance\n","\n","# --------- Load dataset ----------\n","path_enriched = '/content/weather_with_advanced_clusters.csv'\n","path_orig = '/conent/weather_forecast_data.csv'\n","\n","if os.path.exists(path_enriched):\n","    df = pd.read_csv(path_enriched)\n","    print(\"Loaded:\", path_enriched)\n","elif os.path.exists(path_orig):\n","    df = pd.read_csv(path_orig)\n","    print(\"Loaded:\", path_orig)\n","else:\n","    raise FileNotFoundError(\"No dataset found at /mnt/data. Upload your CSV there.\")\n","\n","# If you have a Series `y` in the notebook and df lacks 'Rain', attach it\n","try:\n","    if 'y' in globals() and 'Rain' not in df.columns:\n","        if len(y) == len(df):\n","            df['Rain'] = y.values\n","            print(\"Attached 'y' Series as df['Rain'].\")\n","        else:\n","            print(\"Found 'y' Series but length != df; not attached.\")\n","except Exception as e:\n","    print(\"Skipping attach of 'y':\", e)\n","\n","# --------- Ensure target exists ----------\n","if 'Rain' in df.columns and 'Rain_Encoded' not in df.columns:\n","    df['Rain_Encoded'] = df['Rain'].map({'rain': 1, 'no rain': 0})\n","    if df['Rain_Encoded'].isnull().any():\n","        df['Rain_Encoded'] = df['Rain'].astype('category').cat.codes\n","    print(\"Created 'Rain_Encoded'.\")\n","\n","if 'Rain_Encoded' not in df.columns:\n","    raise ValueError(\"No 'Rain_Encoded' target found. Make sure df has 'Rain' or attach 'y' Series.\")\n","\n","# --------- Prepare features & target ----------\n","target = 'Rain_Encoded'\n","X = df.drop(columns=[target, 'Rain'], errors='ignore')\n","X = X.select_dtypes(include=[np.number])   # numeric features only\n","y_enc = df[target].astype(int)\n","\n","print(\"Features:\", X.columns.tolist())\n","print(\"Class counts:\\n\", y_enc.value_counts())\n","\n","# --------- Train/test split ----------\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y_enc, test_size=0.20, random_state=42, stratify=y_enc\n",")\n","\n","# --------- Scale features ----------\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# --------- MLP + hyperparameter distribution ----------\n","mlp = MLPClassifier(max_iter=500, early_stopping=True, n_iter_no_change=20, random_state=42)\n","\n","param_dist = {\n","    'hidden_layer_sizes': [(50,), (100,), (50,30), (100,50), (150,100,50)],\n","    'activation': ['relu', 'tanh'],\n","    'alpha': [1e-4, 1e-3, 1e-2, 1e-1],\n","    'learning_rate_init': [1e-3, 5e-4, 1e-4],\n","    'solver': ['adam', 'sgd'],\n","    'learning_rate': ['constant', 'adaptive']\n","}\n","\n","cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n","\n","# n_iter_search: how many parameter settings to try. Reduce to 8-12 for faster runs.\n","n_iter_search = 24\n","\n","random_search = RandomizedSearchCV(\n","    estimator=mlp,\n","    param_distributions=param_dist,\n","    n_iter=n_iter_search,\n","    scoring='f1',\n","    n_jobs=-1,\n","    cv=cv,\n","    random_state=42,\n","    verbose=2,\n","    refit=True\n",")\n","\n","# --------- Run search ----------\n","start = time.time()\n","random_search.fit(X_train_scaled, y_train)\n","elapsed = time.time() - start\n","print(f\"Search finished in {elapsed:.1f}s. Best CV F1: {random_search.best_score_:.4f}\")\n","print(\"Best params:\", random_search.best_params_)\n","\n","best_mlp = random_search.best_estimator_\n","\n","# --------- Evaluate on held-out test set ----------\n","y_pred = best_mlp.predict(X_test_scaled)\n","acc = accuracy_score(y_test, y_pred)\n","print(f\"\\nTuned MLP Test Accuracy: {acc:.4f}\\n\")\n","print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n","\n","cm = confusion_matrix(y_test, y_pred)\n","print(\"Confusion matrix:\\n\", cm)\n","\n","# --------- Confusion matrix plot (matplotlib) ----------\n","plt.figure(figsize=(5,5))\n","plt.imshow(cm, interpolation='nearest')\n","plt.title(\"Confusion Matrix - Tuned MLP\")\n","plt.colorbar()\n","classes = ['No Rain','Rain'] if set(y_enc) <= {0,1} else [str(c) for c in sorted(set(y_enc))]\n","plt.xticks(range(len(classes)), classes)\n","plt.yticks(range(len(classes)), classes)\n","plt.xlabel('Predicted')\n","plt.ylabel('Actual')\n","for i in range(cm.shape[0]):\n","    for j in range(cm.shape[1]):\n","        plt.text(j, i, cm[i,j], ha='center', va='center')\n","plt.show()\n","\n","# --------- ROC & AUC (binary) ----------\n","if len(set(y_enc)) == 2:\n","    y_prob = best_mlp.predict_proba(X_test_scaled)[:,1]\n","    try:\n","        auc = roc_auc_score(y_test, y_prob)\n","        print(f\"ROC AUC: {auc:.4f}\")\n","        fpr, tpr, _ = roc_curve(y_test, y_prob)\n","        plt.figure(figsize=(6,5))\n","        plt.plot(fpr, tpr)\n","        plt.plot([0,1],[0,1], linestyle='--')\n","        plt.title('ROC Curve - Tuned MLP')\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.show()\n","    except Exception as e:\n","        print(\"ROC AUC failed:\", e)\n","\n","# --------- Training loss curve (if available) ----------\n","if hasattr(best_mlp, 'loss_curve_') and len(getattr(best_mlp, 'loss_curve_'))>0:\n","    plt.figure(figsize=(8,4))\n","    plt.plot(best_mlp.loss_curve_)\n","    plt.title('MLP training loss curve')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.show()\n","else:\n","    print(\"No loss_curve_ available for this estimator.\")\n","\n","# --------- Permutation feature importance (on test set) ----------\n","print(\"Computing permutation importance (may take a little while)...\")\n","perm = permutation_importance(best_mlp, X_test_scaled, y_test, n_repeats=20, random_state=42, n_jobs=-1)\n","importances = perm.importances_mean\n","indices = np.argsort(importances)[::-1]\n","top_n = min(20, len(importances))\n","\n","plt.figure(figsize=(8, max(4, top_n*0.3)))\n","plt.bar(range(top_n), importances[indices][:top_n])\n","plt.xticks(range(top_n), [X.columns[i] for i in indices[:top_n]], rotation=90)\n","plt.title('Permutation feature importance (MLP on test set)')\n","plt.tight_layout()\n","plt.show()\n","\n","# --------- Save tuned model ----------\n","out = {\n","    'model': best_mlp,\n","    'scaler': scaler,\n","    'features': X.columns.tolist(),\n","    'best_params': random_search.best_params_,\n","    'cv_results': random_search.cv_results_\n","}\n","joblib.dump(out, '/content/tuned_mlp_model.joblib')\n","print(\"Saved tuned MLP to /content/tuned_mlp_model.joblib\")\n","\n","# --------- Quick comparison table (if previous models exist) ----------\n","summary = []\n","def load_saved(path):\n","    if os.path.exists(path):\n","        return joblib.load(path)\n","    return None\n","\n","for name, path in [\n","    ('Decision Tree','/content/decision_tree_model.joblib'),\n","    ('Random Forest','/content/random_forest_model.joblib'),\n","    ('LogReg','/content/logistic_regression_model.joblib')]:\n","    data = load_saved(path)\n","    if data is not None:\n","        m = data.get('model')\n","        s = data.get('scaler')\n","        f = data.get('features')\n","        X_test_local = X_test.copy()\n","        if f is not None:\n","            X_test_local = X_test[f]\n","        Xt = s.transform(X_test_local) if s is not None else X_test_local.values\n","        try:\n","            p = m.predict(Xt)\n","            summary.append((name, accuracy_score(y_test, p)))\n","        except Exception:\n","            pass\n","\n","summary.append(('Tuned MLP', acc))\n","print(\"\\nModel accuracies on the same held-out test set:\")\n","for n,a in summary:\n","    print(f\"{n}: {a:.4f}\")\n","\n","# optional: show a few test rows with MLP predictions\n","sample_idx = np.random.choice(X_test.index, size=min(8, len(X_test)), replace=False)\n","sample = X.loc[sample_idx].copy()\n","sample['true_Rain'] = y_enc.loc[sample_idx].values\n","sample['pred_Rain_MLP'] = best_mlp.predict(scaler.transform(sample[X.columns]))\n","print(\"\\nSample MLP predictions:\")\n","print(sample.head(8).to_string())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9457,"status":"aborted","timestamp":1765927168583,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"},"user_tz":-120},"id":"4m5c3kD9TWxq"},"outputs":[],"source":["# Model comparison cell — paste into Colab and run\n","import os, joblib, numpy as np, pandas as pd, matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n","\n","# Paths to saved models (adjust if you saved with different names)\n","model_paths = {\n","    'Logistic Regression': '/content/logistic_regression_model.joblib',\n","    'Decision Tree': '/content/decision_tree_model.joblib',\n","    'Random Forest': '/content/random_forest_model.joblib',\n","    'Tuned MLP': '/content/tuned_mlp_model.joblib'\n","}\n","\n","# 1) Load dataset\n","path_enriched = '/content/weather_with_advanced_clusters.csv'\n","path_orig = '/content/weather_forecast_data.csv'\n","if os.path.exists(path_enriched):\n","    df = pd.read_csv(path_enriched)\n","    print(\"Loaded:\", path_enriched)\n","elif os.path.exists(path_orig):\n","    df = pd.read_csv(path_orig)\n","    print(\"Loaded:\", path_orig)\n","else:\n","    raise FileNotFoundError(\"No dataset found at /mnt/data. Upload your CSV there.\")\n","\n","# If 'y' Series exists in the notebook and df lacks 'Rain', attach it\n","try:\n","    if 'y' in globals() and 'Rain' not in df.columns:\n","        if len(y) == len(df):\n","            df['Rain'] = y.values\n","            print(\"Attached existing 'y' Series as df['Rain'].\")\n","except Exception:\n","    pass\n","\n","# Ensure target exists\n","if 'Rain' in df.columns and 'Rain_Encoded' not in df.columns:\n","    df['Rain_Encoded'] = df['Rain'].map({'rain':1, 'no rain':0})\n","    if df['Rain_Encoded'].isnull().any():\n","        df['Rain_Encoded'] = df['Rain'].astype('category').cat.codes\n","if 'Rain_Encoded' not in df.columns:\n","    raise ValueError(\"No 'Rain_Encoded' target found. Make sure df has 'Rain' or attach 'y' Series.\")\n","\n","# Select numeric features only (drop target and raw 'Rain')\n","target = 'Rain_Encoded'\n","X = df.drop(columns=[target, 'Rain'], errors='ignore').select_dtypes(include=[np.number])\n","y = df[target].astype(int)\n","\n","# Train/test split (same seed as earlier)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n","\n","# Global scaler for fallback\n","global_scaler = StandardScaler().fit(X_train)\n","\n","def prepare_inputs(saved):\n","    \"\"\"\n","    Given a joblib-loaded object (dict or model), return (X_test_transformed, feature_list, model_obj)\n","    \"\"\"\n","    model_obj = None\n","    scaler = None\n","    features = None\n","    if isinstance(saved, dict):\n","        model_obj = saved.get('model', None)\n","        scaler = saved.get('scaler', None)\n","        features = saved.get('features', None)\n","    else:\n","        model_obj = saved\n","\n","    Xt = X_test.copy()\n","    if features is not None:\n","        features_present = [f for f in features if f in Xt.columns]\n","        Xt = Xt[features_present]\n","    # scaling\n","    if scaler is not None:\n","        try:\n","            Xt_trans = scaler.transform(Xt)\n","        except Exception:\n","            Xt_trans = global_scaler.transform(Xt)\n","    else:\n","        try:\n","            Xt_trans = global_scaler.transform(Xt)\n","        except Exception:\n","            Xt_trans = Xt.values\n","    return Xt_trans, Xt.columns.tolist(), model_obj\n","\n","# Evaluate each saved model\n","results = []\n","roc_curves = {}\n","for name, path in model_paths.items():\n","    if not os.path.exists(path):\n","        print(f\"Model file not found (skipping): {name} -> {path}\")\n","        continue\n","    try:\n","        saved = joblib.load(path)\n","    except Exception as e:\n","        print(f\"Failed to load {path}: {e}\")\n","        continue\n","\n","    X_test_in, features_used, model_obj = prepare_inputs(saved)\n","    if model_obj is None:\n","        print(f\"No model object found inside {path} (skipping).\")\n","        continue\n","\n","    # Predictions\n","    try:\n","        y_pred = model_obj.predict(X_test_in)\n","    except Exception as e:\n","        print(f\"Prediction failed for {name}: {e}\")\n","        continue\n","\n","    # Probabilities for ROC if possible\n","    y_prob = None\n","    try:\n","        if hasattr(model_obj, \"predict_proba\"):\n","            y_prob = model_obj.predict_proba(X_test_in)[:,1]\n","        elif hasattr(model_obj, \"decision_function\"):\n","            df_scores = model_obj.decision_function(X_test_in)\n","            y_prob = (df_scores - df_scores.min()) / (df_scores.max() - df_scores.min() + 1e-12)\n","    except Exception:\n","        y_prob = None\n","\n","    acc = accuracy_score(y_test, y_pred)\n","    prec = precision_score(y_test, y_pred, zero_division=0)\n","    rec = recall_score(y_test, y_pred, zero_division=0)\n","    f1 = f1_score(y_test, y_pred, zero_division=0)\n","    auc = None\n","    if y_prob is not None and len(np.unique(y_test)) == 2:\n","        try:\n","            auc = roc_auc_score(y_test, y_prob)\n","            fpr, tpr, _ = roc_curve(y_test, y_prob)\n","            roc_curves[name] = (fpr, tpr)\n","        except Exception:\n","            auc = None\n","\n","    cm = confusion_matrix(y_test, y_pred)\n","\n","    results.append({\n","        'model': name,\n","        'accuracy': acc,\n","        'precision_rain': prec,\n","        'recall_rain': rec,\n","        'f1_rain': f1,\n","        'roc_auc': auc if auc is not None else np.nan,\n","        'confusion_matrix': cm,\n","        'features_used': features_used\n","    })\n","\n","# Build results DataFrame and save\n","res_df = pd.DataFrame([{\n","    'Model': r['model'],\n","    'Accuracy': r['accuracy'],\n","    'Precision_Rain': r['precision_rain'],\n","    'Recall_Rain': r['recall_rain'],\n","    'F1_Rain': r['f1_rain'],\n","    'ROC_AUC': r['roc_auc']\n","} for r in results])\n","\n","res_df.to_csv('/content/model_comparison.csv', index=False)\n","print(\"Saved comparison to /content/model_comparison.csv\")\n","\n","# Print or display the results\n","try:\n","    from ace_tools import display_dataframe_to_user\n","    display_dataframe_to_user(\"Model comparison\", res_df)\n","except Exception:\n","    print(res_df.to_string())\n","\n","# ---------- Plots (one figure each) ----------\n","if not res_df.empty:\n","    # Accuracy bar chart\n","    plt.figure(figsize=(6,4))\n","    plt.bar(res_df['Model'], res_df['Accuracy'])\n","    plt.title(\"Model Accuracy\")\n","    plt.ylabel(\"Accuracy\")\n","    plt.ylim(0,1.02)\n","    for i,v in enumerate(res_df['Accuracy']):\n","        plt.text(i, v+0.01, f\"{v:.3f}\", ha='center')\n","    plt.show()\n","\n","    # Precision / Recall / F1 grouped bars\n","    x = np.arange(len(res_df['Model']))\n","    width = 0.2\n","    plt.figure(figsize=(8,4))\n","    plt.bar(x - width, res_df['Precision_Rain'], width)\n","    plt.bar(x, res_df['Recall_Rain'], width)\n","    plt.bar(x + width, res_df['F1_Rain'], width)\n","    plt.title(\"Precision / Recall / F1 for 'Rain'\")\n","    plt.xticks(x, res_df['Model'])\n","    plt.ylim(0,1.02)\n","    plt.legend(['Precision','Recall','F1'])\n","    plt.show()\n","\n","    # ROC curves\n","    if roc_curves:\n","        plt.figure(figsize=(6,5))\n","        for name,(fpr,tpr) in roc_curves.items():\n","            plt.plot(fpr, tpr, label=name)\n","        plt.plot([0,1],[0,1], linestyle='--')\n","        plt.title(\"ROC Curves\")\n","        plt.xlabel(\"False Positive Rate\")\n","        plt.ylabel(\"True Positive Rate\")\n","        plt.legend()\n","        plt.show()\n","\n","    # Confusion matrices separately\n","    for r in results:\n","        cm = r['confusion_matrix']\n","        plt.figure(figsize=(4,4))\n","        plt.imshow(cm, interpolation='nearest')\n","        plt.title(f\"Confusion Matrix - {r['model']}\")\n","        plt.colorbar()\n","        classes = ['No Rain','Rain'] if set(y) <= {0,1} else [str(c) for c in sorted(set(y))]\n","        plt.xticks(range(len(classes)), classes)\n","        plt.yticks(range(len(classes)), classes)\n","        plt.xlabel('Predicted')\n","        plt.ylabel('Actual')\n","        for i in range(cm.shape[0]):\n","            for j in range(cm.shape[1]):\n","                plt.text(j, i, cm[i, j], ha='center', va='center')\n","        plt.show()\n","\n","print(\"Comparison complete. model_comparison.csv saved.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QyVE3zxAXB35","executionInfo":{"status":"aborted","timestamp":1765927168589,"user_tz":-120,"elapsed":9463,"user":{"displayName":"hana elzarka","userId":"15108554713947021918"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}